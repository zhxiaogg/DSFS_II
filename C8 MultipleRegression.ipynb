{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C8 Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Regularization: L1-norm, L2-norm penalty, https://zhuanlan.zhihu.com/p/62457875\n",
    "- p-test vs. z-test: \n",
    "    - https://towardsdatascience.com/statistical-tests-when-to-use-which-704557554740\n",
    "    - https://www.youtube.com/watch?v=pTmLQvMM-1M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.51417148  0.9748608  -1.85066841  0.91456421]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Original Dataset\n",
    "# alpha (constant 1), num friends, working hours, is phd?\n",
    "inputs = [[1.,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]\n",
    "daily_minutes = [1,68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]\n",
    "num_friends = [100.0,49,41,40,25,21,21,19,19,18,18,16,15,15,15,15,14,14,13,13,13,13,12,12,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "# see official repo for why it's outlier\n",
    "outlier = num_friends.index(100)\n",
    "\n",
    "# input dataset for this chapter\n",
    "inputs = np.array(inputs)\n",
    "outputs = np.array([x for i, x in enumerate(daily_minutes) if i != outlier])\n",
    "\n",
    "# x*b + c\n",
    "def predict(inputs_x, beta):\n",
    "    return np.dot(inputs_x, beta)\n",
    "\n",
    "def error(inputs_x, inputs_y, beta):\n",
    "    return predict(inputs_x, beta) - inputs_y\n",
    "\n",
    "def sqrt_gradient(inputs_x, inputs_y, beta):\n",
    "    errors = error(inputs_x, inputs_y, beta)\n",
    "    return 2 * errors.reshape((errors.size,1)) * inputs_x\n",
    "\n",
    "# least square fit with mini batch\n",
    "def least_square_fit(inputs_x, inputs_y, beta, learning_rate, num_steps, batch_size):\n",
    "    for i in range(num_steps):\n",
    "        for j in range(0, inputs_y.size, batch_size):\n",
    "            x = inputs_x[j:j+batch_size]\n",
    "            y = inputs_y[j:j+batch_size]\n",
    "            gradients = sqrt_gradient(x, y, beta)\n",
    "            mean_gradients = gradients.mean(axis=0)\n",
    "            beta = beta - learning_rate * mean_gradients\n",
    "    return beta\n",
    "\n",
    "\n",
    "beta = [random.random(), random.random(), random.random(), random.random()]\n",
    "learning_rate = 0.001\n",
    "num_steps = 5000\n",
    "result = least_square_fit(inputs, outputs, beta, learning_rate, num_steps, 25)\n",
    "\n",
    "print(result)\n",
    "# assertions from official repo\n",
    "assert 30.50 < result[0] < 30.70  # constant\n",
    "assert  0.96 < result[1] <  1.00  # num friends\n",
    "assert -1.89 < result[2] < -1.85  # work hours per day\n",
    "assert  0.91 < result[3] <  0.94  # has PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p_value test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scratch.bootstrap as bootstrap\n",
    "import numpy as np\n",
    "\n",
    "def estimate_sample_beta(inputs):\n",
    "    cols = np.size(inputs, 1)\n",
    "    beta = [random.random(), random.random(), random.random(), random.random()]\n",
    "    learning_rate = 0.001\n",
    "    num_steps = 8000\n",
    "    beta = least_square_fit(inputs[..., 0:cols - 1], inputs[..., cols - 1], beta, learning_rate, num_steps, 25)\n",
    "    return beta\n",
    "\n",
    "inputs_data = np.hstack((inputs, outputs.reshape(outputs.size, 1)))\n",
    "results = bootstrap.bootstrap_statistic(inputs_data, estimate_sample_beta, 100)\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.27559426  0.98594285 -1.82030071  1.07602186]\n",
      "[1.67110596 0.14952827 0.1686273  1.3113623 ]\n"
     ]
    }
   ],
   "source": [
    "means = np.median(results, 0)\n",
    "stderrs = np.sqrt(np.sum(((results - means) ** 2), 0) / np.size(results, 0))\n",
    "print(means)\n",
    "print(stderrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 4.29030144744047e-11, 0.0, 0.41190979829094454]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def normal_cdf(x: float, mu: float = 0, sigma:float = 1) -> float:\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "def p_value(beta:float, sigma:float) -> float:\n",
    "    if (beta > 0):\n",
    "        return 2 * (1 - normal_cdf(beta/sigma))\n",
    "    else:\n",
    "        return 2 * normal_cdf(beta/sigma)\n",
    "    \n",
    "betaSigmas = np.hstack((means.reshape(means.size, 1), stderrs.reshape(stderrs.size, 1)))\n",
    "\n",
    "pValues = [p_value(e[0], e[1]) for e in betaSigmas]\n",
    "print(pValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "**ridge regression vs. lasso regression**:\n",
    "- the ridge regression shranks the coefficients overall\n",
    "- the lasso regression tends to force coefficients to be 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# alpha * beta^2\n",
    "# gradient: 2 * alpha * beta\n",
    "def ridge_penalty(beta, alpha: float) -> float:\n",
    "    return alpha * np.dot(beta, beta)\n",
    "\n",
    "# alpha * sum(|beta|)\n",
    "# no gradient, thus cannot work with gradient descent methods\n",
    "def lasso_penalty(beta, alpha: float) -> float:\n",
    "    return alpha * np.sum(np.abs(beta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
